\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\citation{*}
\citation{HornikEtAl89}
\citation{MachineLearningProjects_2023}
\@writefile{toc}{\contentsline {section}{Introduction}{3}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Gradient Descent}{4}{section.0.1}\protected@file@percent }
\newlabel{sec:GD}{{1}{4}{Introduction}{section.0.1}{}}
\newlabel{foot:gpu}{{1}{4}{Introduction}{section.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Backpropagation and Chain Rule}{5}{subsection.0.1.1}\protected@file@percent }
\newlabel{sec:backpropagation}{{1.1}{5}{Introduction}{subsection.0.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the Chain Rule in Forward and Backward Passes\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chainrule}{{1}{5}{Illustration of the Chain Rule in Forward and Backward Passes\relax }{figure.caption.3}{}}
\citation{neutelings_tikzcode}
\citation{neutelings_tikzcode}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{6}{section.0.2}\protected@file@percent }
\newlabel{sec:NN}{{2}{6}{Introduction}{section.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A model of a neural network with one hidden layer consisting of four nodes\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:nn}{{2}{6}{A model of a neural network with one hidden layer consisting of four nodes\relax }{figure.caption.4}{}}
\citation{misc_breast_cancer17}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of how a single node gets its value during a forward pass. This shows that matrix operations can be used for more efficient calculations. When $a$ is a batch of inputs, these calculations become matrix-matrix multiplications, significantly speeding up both training and inference.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nn_math}{{3}{7}{Illustration of how a single node gets its value during a forward pass. This shows that matrix operations can be used for more efficient calculations. When $a$ is a batch of inputs, these calculations become matrix-matrix multiplications, significantly speeding up both training and inference.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Data}{7}{section.0.3}\protected@file@percent }
\newlabel{sec:data}{{3}{7}{Introduction}{section.0.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Feature correlation matrix. This provides insight into the redundancy among features. Only every other feature is annotated, but the missing labels can be inferred from Figure\nobreakspace  {}\ref {fig:feature_histogram}.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:feature_correlation}{{4}{8}{Feature correlation matrix. This provides insight into the redundancy among features. Only every other feature is annotated, but the missing labels can be inferred from Figure~\ref {fig:feature_histogram}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Feature histogram. The red class ($0$) represents malignant cases, while the green class ($1$) indicates benign cases.\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:feature_histogram}{{5}{9}{Feature histogram. The red class ($0$) represents malignant cases, while the green class ($1$) indicates benign cases.\relax }{figure.caption.7}{}}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Analysis}{10}{section.0.4}\protected@file@percent }
\newlabel{sec:resultsdiscussion}{{4}{10}{Introduction}{section.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Hyperparameters}{10}{subsection.0.4.1}\protected@file@percent }
\newlabel{sec:hyperparameters}{{4.1}{10}{Introduction}{subsection.0.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Accuracy versus learning rate and momentum. \relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:accuracy_lr_gamma}{{6}{10}{Accuracy versus learning rate and momentum. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Accuracy versus regualarization. \relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:accuracy_aplha}{{7}{10}{Accuracy versus regualarization. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Model accuracy across different optimizers. here sgd is standard gradient descent. \relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:accuracy_optimizer}{{8}{11}{Model accuracy across different optimizers. here sgd is standard gradient descent. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Accuracy based on batch size. \relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:accuracy_batch}{{9}{11}{Accuracy based on batch size. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Model performance with different activation functions. \relax }}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:accuracy_activ}{{10}{12}{Model performance with different activation functions. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Accuracy in relation to the number of layers and nodes. \relax }}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:accuracy_layers_nodes}{{11}{12}{Accuracy in relation to the number of layers and nodes. \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Final Evaluation and Comparisons}{12}{subsection.0.4.2}\protected@file@percent }
\newlabel{sec:comparisons}{{4.2}{12}{Introduction}{subsection.0.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Confusion matrix for our model. \relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{12}{12}{Confusion matrix for our model. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Confusion matrix for SKLearn's model. \relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:confusion_matrix_sklearn}{{13}{12}{Confusion matrix for SKLearn's model. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{13}{section.0.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{13}{Introduction}{section.0.5}{}}
\citation{MachineLearningProjects_2023}
\@writefile{toc}{\contentsline {chapter}{Appendix}{14}{section*.12}\protected@file@percent }
\newlabel{app:appendixA}{{5}{14}{Appendix A}{chapter*.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Regression with Neural Networks}{14}{section.appendixA}\protected@file@percent }
\newlabel{sec:regression}{{A}{14}{Appendix A}{section.appendixA}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Sample visualization of Perlin noise. This noise function generates a variety of gradients, creating a natural, smooth variation in data.\relax }}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:perlin}{{14}{14}{Sample visualization of Perlin noise. This noise function generates a variety of gradients, creating a natural, smooth variation in data.\relax }{figure.caption.15}{}}
\newlabel{sec:resultsdiscussion2}{{A}{15}{Results and Analysis}{section*.16}{}}
\newlabel{sec:hyperparameters2}{{A}{15}{Hyperparameters}{section*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces MSE versus learning rate and momentum \relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:MSE_lr_gamma}{{15}{15}{MSE versus learning rate and momentum \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  MSE versus regualarization. \relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:MSE_aplha}{{16}{15}{MSE versus regualarization. \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Model performance across different optimizers. here sgd is standard gradient descent. \relax }}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MSE_optimizer}{{17}{16}{Model performance across different optimizers. here sgd is standard gradient descent. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  MSE based on batch size. \relax }}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MSE_batch}{{18}{16}{MSE based on batch size. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Model performance with different activation functions. \relax }}{16}{figure.caption.20}\protected@file@percent }
\newlabel{fig:MSE_activ}{{19}{16}{Model performance with different activation functions. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  MSE in relation to the number of layers and nodes. \relax }}{16}{figure.caption.20}\protected@file@percent }
\newlabel{fig:MSE_layers_nodes}{{20}{16}{MSE in relation to the number of layers and nodes. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  MSE in relation to the polynomial degree. \relax }}{17}{figure.caption.21}\protected@file@percent }
\newlabel{fig:}{{21}{17}{MSE in relation to the polynomial degree. \relax }{figure.caption.21}{}}
\newlabel{sec:comparisons2}{{A}{17}{Final Evaluation and Comparisons}{section*.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Logistic regression model predictions. \relax }}{17}{figure.caption.23}\protected@file@percent }
\newlabel{fig:perlinNoise_logistic_pred}{{22}{17}{Logistic regression model predictions. \relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Neural network model predictions. \relax }}{17}{figure.caption.23}\protected@file@percent }
\newlabel{fig:perlinNoise_NN_pred}{{23}{17}{Neural network model predictions. \relax }{figure.caption.23}{}}
\citation{HornikEtAl89}
\newlabel{app:appendixB}{{A}{19}{Appendix B}{chapter*.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Universal Approximation}{19}{section.appendixB}\protected@file@percent }
\newlabel{sec:UAT}{{B}{19}{Appendix B}{section.appendixB}{}}
\newlabel{app:xor}{{B}{19}{XOR VS Perceptron}{section*.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  XOR-like dataset. The two classes are not linearly separable. \relax }}{19}{figure.caption.26}\protected@file@percent }
\newlabel{fig:xor_data}{{24}{19}{XOR-like dataset. The two classes are not linearly separable. \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  The output of a logistic regression model with no polynomial features added to the data. We get a classification accuracy of 0.5. \relax }}{20}{figure.caption.27}\protected@file@percent }
\newlabel{fig:xor_plain}{{25}{20}{The output of a logistic regression model with no polynomial features added to the data. We get a classification accuracy of 0.5. \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  The output of a logistic regression model with polynomial features added to the data. We get a classification accuracy of 1.0. \relax }}{20}{figure.caption.27}\protected@file@percent }
\newlabel{fig:xor_poly}{{26}{20}{The output of a logistic regression model with polynomial features added to the data. We get a classification accuracy of 1.0. \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces  The output of a neural network with no polynomial features added to the data. We get a classification accuracy of 1.0. \relax }}{20}{figure.caption.28}\protected@file@percent }
\newlabel{fig:xor_nn}{{27}{20}{The output of a neural network with no polynomial features added to the data. We get a classification accuracy of 1.0. \relax }{figure.caption.28}{}}
\citation{nielsenneural}
\bibdata{report}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The $\text  {sin}$ function to be approximated.\relax }}{21}{figure.caption.30}\protected@file@percent }
\newlabel{fig:sin}{{28}{21}{The $\text {sin}$ function to be approximated.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Combining outputs of individual neurons to approximate a $\text  {sin}$ function.\relax }}{21}{figure.caption.31}\protected@file@percent }
\newlabel{fig:universal}{{29}{21}{Combining outputs of individual neurons to approximate a $\text {sin}$ function.\relax }{figure.caption.31}{}}
\newlabel{app:neuronscombined}{{B}{21}{Combining Neurons}{section*.29}{}}
\bibcite{Goodfellow-et-al-2016}{1}
\bibcite{hastie01statisticallearning}{2}
\bibcite{fys-stk}{3}
\bibcite{HornikEtAl89}{4}
\bibcite{neutelings_tikzcode}{5}
\bibcite{nielsenneural}{6}
\bibcite{gpt}{7}
\bibcite{scikit-learn}{8}
\bibcite{MachineLearningProjects_2023}{9}
\bibcite{misc_breast_cancer17}{10}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{Bibliography}{22}{chapter*.32}\protected@file@percent }
\gdef \@abspage@last{22}
